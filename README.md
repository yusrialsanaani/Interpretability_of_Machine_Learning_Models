# Machine Learning Models Interpretability

The **objective** of this project is to introduce the explainable AI methods such as **SHAP** and **LIME**. These methods allow to explain and interpret the predictions of machine learning models, hence providing insight into the criteria behind predictions of machine learning models.

The main **Importance of Interpretability** is to allow us to perform error analysis and building a trust.
- Perform Error Analysis: allows to work specifically on weak points and to counteract them when training a model.
- Building a trust: many machine learning applications require trust in them so that the application can help with making a decision. Knowing the reasons for a prediction makes the decision to trust the application easier.

## Dataset
The dataset used in this project is Parkinsons Data Set taken from https://archive.ics.uci.edu/ml/datasets/parkinsons.
- This dataset is based on voice measures in frequency. 
- It was produced and released by the University of Oxford, available in the UCI ML Repository.Â 
- The dataset consists of 22 features and 1 result column of the diagnosis. 
- There are a total of 195 rows. Moreover, the dataset has been created from a measure of dysphonia and pitch period entropy.



